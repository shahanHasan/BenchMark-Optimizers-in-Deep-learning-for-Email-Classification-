{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamBase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdJdCN2UJT9"
      },
      "source": [
        "# LSTM-BI-LSTM-GRU-BI-GRU-RNN-BRNN \n",
        "# ALL Optimizers\n",
        "# SpamBase Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaH2j9Of0M2O",
        "outputId": "3f0b81b5-27e7-4d6b-f478-ac46ddbd0b61"
      },
      "source": [
        "!pip3 install tensorflow-addons\n",
        "!pip3 install keras-adabound\n",
        "!pip3 install adabelief-tf==0.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n",
            "Collecting keras-adabound\n",
            "  Downloading keras-adabound-0.6.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.7.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.7.1)\n",
            "Building wheels for collected packages: keras-adabound\n",
            "  Building wheel for keras-adabound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-adabound: filename=keras_adabound-0.6.0-py3-none-any.whl size=6607 sha256=c1202386efd682f9ddf070430565ec5801d0789563b2fafc162c05533f5962f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/bf/39/3d95847ef12aa58c15a6cc7a20f4f21ea39fcd52793e1beea0\n",
            "Successfully built keras-adabound\n",
            "Installing collected packages: keras-adabound\n",
            "Successfully installed keras-adabound-0.6.0\n",
            "Collecting adabelief-tf==0.2.0\n",
            "  Downloading adabelief_tf-0.2.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf==0.2.0) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf==0.2.0) (0.8.9)\n",
            "Collecting colorama>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.41.1)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.1.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.21.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.0.0->adabelief-tf==0.2.0) (3.6.0)\n",
            "Installing collected packages: colorama, adabelief-tf\n",
            "Successfully installed adabelief-tf-0.2.0 colorama-0.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujift3_2UHwV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Time and counters\n",
        "from time import perf_counter\n",
        "\n",
        "# Optimizers\n",
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Nadam, Adadelta, Adamax, Ftrl\n",
        "from adabelief_tf import AdaBeliefOptimizer\n",
        "from keras_adabound import AdaBound\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Lambda, Dense, Embedding, Bidirectional,concatenate\n",
        "from tensorflow.keras.layers import Dropout, Input,InputLayer, ReLU, LSTM\n",
        "from tensorflow.keras.layers import GRU, SimpleRNN\n",
        "# from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import f1_score , recall_score, accuracy_score, precision_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Mark Down print\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTy7v6OufS6g",
        "outputId": "070f3788-44af-446a-fa32-df8461b956f3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU8hOquu7l-0"
      },
      "source": [
        "# Import Optimizers and other paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmVVtLJD0Ffy"
      },
      "source": [
        "#paths\n",
        "SPAMBASE   = \"drive/MyDrive/Mahboob/Datasets/SpamBase/PROCESSED.csv\" \n",
        "heatmaps   = \"drive/MyDrive/Mahboob/Heatmaps/SpamBase/\"\n",
        "Visuals    = \"drive/MyDrive/Mahboob/Visuals/SPAMBASE/\"\n",
        "Comparison = \"drive/MyDrive/Mahboob/Comparison/\"\n",
        "\n",
        "\n",
        "adabelief      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADABELIEF/\"\n",
        "adabound      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADABOUND/\"\n",
        "adadelta      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADADELTA/\"\n",
        "adagrad      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADAGRAD/\"\n",
        "adam      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADAM/\"\n",
        "adamax      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADAMAX/\"\n",
        "adamw      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/ADAMW/\"\n",
        "amsbound      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/AMSBOUND/\"\n",
        "amsgrad      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/AMSGRAD/\"\n",
        "ftrl      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/FTRL/\"\n",
        "nadam      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/NADAM/\"\n",
        "radam      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/RADAM/\"\n",
        "rmsprop      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/RMSPROP/\"\n",
        "sgd      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/SGD/\"\n",
        "sgd_nesterov      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/SGD-NESTEROV/\"\n",
        "sgd_momentum      = \"drive/MyDrive/Mahboob/Models/SPAMBASE/SGD-MOMENTUM/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvHyaK7x3uX6",
        "outputId": "1721c019-5b80-48d6-d909-a5d548956e94"
      },
      "source": [
        "# 1. ALL Optimizer List\n",
        "# Adam weight decay\n",
        "\n",
        "AdamW = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
        "AMSGrad = Adam(amsgrad=True)\n",
        "Rectified_Adam = tfa.optimizers.RectifiedAdam(learning_rate=0.001)\n",
        "\n",
        "SGD_momentum = SGD(momentum=0.9)\n",
        "Nesterov_SGD_momentum = SGD(momentum=0.9, nesterov=True)\n",
        "#Adam #Nadam #SGD #RMSprop #Adadelta #Adagrad #Adamax #Ftrl\n",
        "AdaBelief = AdaBeliefOptimizer(learning_rate=1e-3, epsilon=1e-14, rectify=False)\n",
        "Adabound = AdaBound(lr=1e-3, final_lr=0.1)\n",
        "AMSbound = AdaBound(lr=1e-3, final_lr=0.1, amsgrad=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLiMOQDY8n4y",
        "outputId": "72520d21-095d-4aa8-d21d-57eea7d1564d"
      },
      "source": [
        "# drive.mount(\"/content/drive\", force_remount=True)\n",
        "import sys\n",
        "sys.path.append(\"drive/MyDrive/Mahboob/\")\n",
        "!ls\n",
        "from utils import *"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  -------------\n",
            "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
            ">=0.1.0 (Current 0.2.0)  1e-14  supported          default: True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oZab7PW7rer"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX0qvpGc6iMQ"
      },
      "source": [
        "data = pd.read_csv(SPAMBASE)\n",
        "Label = data.label.copy()\n",
        "data = data.drop(columns=\"label\")\n",
        "# data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
        "# data.to_csv(ENRON, index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "mzQc3WUQ6Xea",
        "outputId": "e5825e35-8406-4d83-f930-3bdc2b8f7161"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  ...  capital_run_length_total\n",
              "0            0.21  ...                      1028\n",
              "1            0.06  ...                      2259\n",
              "2            0.00  ...                       191\n",
              "3            0.00  ...                       191\n",
              "4            0.00  ...                        54\n",
              "\n",
              "[5 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "6PuFfTUI3DBV",
        "outputId": "e3ced929-7287-4ac6-8366-51e92653665d"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104576</td>\n",
              "      <td>0.212922</td>\n",
              "      <td>0.280578</td>\n",
              "      <td>0.065439</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.095922</td>\n",
              "      <td>0.114233</td>\n",
              "      <td>0.105317</td>\n",
              "      <td>0.090087</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.059837</td>\n",
              "      <td>0.541680</td>\n",
              "      <td>0.093950</td>\n",
              "      <td>0.058639</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0.248833</td>\n",
              "      <td>0.142617</td>\n",
              "      <td>0.184504</td>\n",
              "      <td>1.662041</td>\n",
              "      <td>0.085596</td>\n",
              "      <td>0.809728</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.094289</td>\n",
              "      <td>0.549624</td>\n",
              "      <td>0.265441</td>\n",
              "      <td>0.767472</td>\n",
              "      <td>0.124872</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.102874</td>\n",
              "      <td>0.064767</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.097250</td>\n",
              "      <td>0.047846</td>\n",
              "      <td>0.105435</td>\n",
              "      <td>0.097498</td>\n",
              "      <td>0.136983</td>\n",
              "      <td>0.013204</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.132367</td>\n",
              "      <td>0.046109</td>\n",
              "      <td>0.079213</td>\n",
              "      <td>0.301289</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.005446</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.038583</td>\n",
              "      <td>0.139061</td>\n",
              "      <td>0.016980</td>\n",
              "      <td>0.268960</td>\n",
              "      <td>0.075827</td>\n",
              "      <td>0.044248</td>\n",
              "      <td>5.191827</td>\n",
              "      <td>52.170870</td>\n",
              "      <td>283.290435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305387</td>\n",
              "      <td>1.290700</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>1.395303</td>\n",
              "      <td>0.672586</td>\n",
              "      <td>0.273850</td>\n",
              "      <td>0.391480</td>\n",
              "      <td>0.401112</td>\n",
              "      <td>0.278643</td>\n",
              "      <td>0.644816</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.861791</td>\n",
              "      <td>0.301065</td>\n",
              "      <td>0.335219</td>\n",
              "      <td>0.258871</td>\n",
              "      <td>0.825881</td>\n",
              "      <td>0.444099</td>\n",
              "      <td>0.530930</td>\n",
              "      <td>1.775669</td>\n",
              "      <td>0.509821</td>\n",
              "      <td>1.200938</td>\n",
              "      <td>1.025866</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.442681</td>\n",
              "      <td>1.671511</td>\n",
              "      <td>0.887043</td>\n",
              "      <td>3.367639</td>\n",
              "      <td>0.538631</td>\n",
              "      <td>0.593389</td>\n",
              "      <td>0.456729</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>0.328594</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.329480</td>\n",
              "      <td>0.532315</td>\n",
              "      <td>0.402664</td>\n",
              "      <td>0.423493</td>\n",
              "      <td>0.220675</td>\n",
              "      <td>0.434718</td>\n",
              "      <td>0.349953</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.766900</td>\n",
              "      <td>0.223835</td>\n",
              "      <td>0.622042</td>\n",
              "      <td>1.011787</td>\n",
              "      <td>0.911214</td>\n",
              "      <td>0.076283</td>\n",
              "      <td>0.285765</td>\n",
              "      <td>0.243497</td>\n",
              "      <td>0.270377</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.815726</td>\n",
              "      <td>0.245906</td>\n",
              "      <td>0.429388</td>\n",
              "      <td>31.732891</td>\n",
              "      <td>194.912453</td>\n",
              "      <td>606.413764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314250</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.705250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>265.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word_freq_make  ...  capital_run_length_total\n",
              "count     4600.000000  ...               4600.000000\n",
              "mean         0.104576  ...                283.290435\n",
              "std          0.305387  ...                606.413764\n",
              "min          0.000000  ...                  1.000000\n",
              "25%          0.000000  ...                 35.000000\n",
              "50%          0.000000  ...                 95.000000\n",
              "75%          0.000000  ...                265.250000\n",
              "max          4.540000  ...              15841.000000\n",
              "\n",
              "[8 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEoiiT573IIh",
        "outputId": "42c9b35f-eb5f-42db-ce9c-293a678d5b7b"
      },
      "source": [
        "len(data.columns)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE529Pay7gg8"
      },
      "source": [
        "# Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1QkPFHO4Hu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e453be-c8b8-4ca2-9e2d-486ced1397b3"
      },
      "source": [
        "#Splitting the data - 80:20 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(data , Label, test_size = 0.2, random_state = 99)\n",
        "print(f\"Training split input: {X_train.shape}\")\n",
        "print(f\"Testing split input : {X_test.shape}\")\n",
        "print(f\"Training split class: {y_train.shape}\")\n",
        "print(f\"Testing split class : {y_test.shape}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training split input: (3680, 57)\n",
            "Testing split input : (920, 57)\n",
            "Training split class: (3680,)\n",
            "Testing split class : (920,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-LlvOia6s4n",
        "outputId": "261b011d-b058-4732-f891-c25c86bc77a1"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3680, 57)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHGgB6St6iUA",
        "outputId": "2e6cf4a6-ae97-4eab-a437-ae339dbd7058"
      },
      "source": [
        "X_train_reshaped = X_train.to_numpy().reshape((1,X_train.shape[0], X_train.shape[1]))\n",
        "# X_train_reshaped = np.expand_dims(X_train, -1)\n",
        "X_train_reshaped.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3680, 57)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVcRKwx08Nz3",
        "outputId": "2aae6137-1004-45c6-8daf-ba2a49ec504b"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(920, 57)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGChpp978RDz",
        "outputId": "a860054e-b63b-4417-a2f4-c0e71f7c887d"
      },
      "source": [
        "# X_test_reshaped = np.expand_dims(X_test, -1)\n",
        "X_test_reshaped = X_test.to_numpy().reshape((1,X_test.shape[0], X_test.shape[1]))\n",
        "X_test_reshaped.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 920, 57)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs06SQzcAGLt"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(100, input_shape=(3680,57))))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "DFpbyco0CLAS",
        "outputId": "f13de896-feb6-409b-8691-3c259cfb439f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2580\u001b[0;31m           \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m           \u001b[0;34m'Build the model first by calling `build()` or by calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m           'the model on a batch of data.')\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "LxR74JvrCP1b",
        "outputId": "63bb4f0a-4b15-4e85-e870-cf3bf6f2dbb7"
      },
      "source": [
        "history = model.fit(X_train_reshaped, y_train, epochs=50)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-23ed82a3f631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1656\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1\n  y sizes: 3680\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV4GJXpd0dt1"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwj5GOeZDw6F"
      },
      "source": [
        "def Model_Train(arc_func, optim_func):\n",
        "  model = arc_func(3680, numFeatures=57)\n",
        "  model.compile(optimizer=optim_func, loss='binary_crossentropy', metrics=['accuracy',])\n",
        "  start = perf_counter()\n",
        "  history = model.fit(X_train_reshaped, y_train, batch_size=512, epochs=10, validation_split=0.2)\n",
        "  duration = perf_counter() - start\n",
        "  duration = round(duration,2)\n",
        "\n",
        "  # Predicting the Test set results\n",
        "  y_pred = model.predict(X_test_reshaped)\n",
        "  y_pred = (y_pred > 0.5)\n",
        "  y_pred = np.array(y_pred)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)\n",
        "  test_err = 100 - test_acc*100\n",
        "\n",
        "  print(f\"Test Loss:     {test_loss*100} %\")\n",
        "  print(f\"Test Accuracy: {test_acc*100}  %\")\n",
        "  print(f\"Test error: {test_err}  %\")\n",
        "\n",
        "  return model, history, duration, y_pred, test_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvQRucJGc4Ba"
      },
      "source": [
        "Optimizer_list = [\"Adam\", \"Nadam\", \"SGD\", \"RMSProp\", \"Adagrad\", \"Adadelta\", \"Ftrl\", \"SGD-Momentum\", \"SGD-Nesterov-Momentum\", \"Adam-Weight-Decay\"] \n",
        "\n",
        "Optimizer_function_list = [Adam(), Nadam(), SGD(), RMSprop(), Adagrad(), Adadelta(), Ftrl(), SGD(momentum=0.9), SGD(momentum=0.9, nesterov=True), tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)]\n",
        "\n",
        "\n",
        "Model_Architectures = {\n",
        "    \"\"\"\n",
        "    Change model Architectures to 1D models like CNN1D, Dense models (ANN), ML-Classifiers etc\n",
        "    \"\"\" \n",
        "    \"BRNN\" : BI_RNN_ARCHITECTURE_SPAMBASE,\n",
        "    \"Bi-LSTM\" : BI_LSTM_ARCHITECTURE_SPAMBASE,\n",
        "    \"Bi-GRU\" : BI_GRU_ARCHITECTURE_SPAMBASE\n",
        "}\n",
        "\n",
        "Deliverables = {\n",
        "    \"Optimizer_history\" : {\n",
        "          \"BRNN\" : [],\n",
        "          \"Bi-LSTM\" : [],\n",
        "          \"Bi-GRU\" : []\n",
        "    },\n",
        "    \"Models\" : {\n",
        "          \"BRNN\" : [],\n",
        "          \"Bi-LSTM\" : [],\n",
        "          \"Bi-GRU\" : []        \n",
        "    },\n",
        "    \"Duration\" : {\n",
        "          \"BRNN\" : [],\n",
        "          \"Bi-LSTM\" : [],\n",
        "          \"Bi-GRU\" : []        \n",
        "    },\n",
        "    \"Y_pred\" : {\n",
        "          \"BRNN\" : [],\n",
        "          \"Bi-LSTM\" : [],\n",
        "          \"Bi-GRU\" : []        \n",
        "    },\n",
        "    \"Test_Loss\" : {\n",
        "          \"BRNN\" : [],\n",
        "          \"Bi-LSTM\" : [],\n",
        "          \"Bi-GRU\" : []        \n",
        "    },\n",
        "}\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5AHENdJTBu8"
      },
      "source": [
        "# BRNN - Bi-LSTM - Bi-GRU\n",
        "## Adam, Nadam, SGD, RMSprop, Adagrad, Adadelta, Ftrl, SGD momentum, SGD Nesterov Momentum, Adamw "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "SvNB59vswfYx",
        "outputId": "e422f121-b5d2-428f-98ec-32436a08315a"
      },
      "source": [
        "for k,v in enumerate(Model_Architectures.items()):\n",
        "  for j in Optimizer_function_list:\n",
        "    print(f\"{k} : {v} , optimizer : {j}\")\n",
        "    model, history, duration, y_pred, test_loss = Model_Train(v[1],j)\n",
        "    Deliverables[\"Optimizer_history\"][v[0]].append(history)\n",
        "    Deliverables[\"Models\"][v[0]].append(model)\n",
        "    Deliverables[\"Duration\"][v[0]].append(duration)\n",
        "    Deliverables[\"Y_pred\"][v[0]].append(y_pred)\n",
        "    Deliverables[\"Test_Loss\"][v[0]].append(test_loss)\n",
        "    print(\"done training\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : ('BRNN', <function BI_RNN_ARCHITECTURE_SPAMBASE at 0x7f26733088c0>) , optimizer : <keras.optimizer_v2.adam.Adam object at 0x7f26731101d0>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d788e12ece3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOptimizer_function_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{k} : {v} , optimizer : {j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mDeliverables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Optimizer_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDeliverables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-76208ccff037>\u001b[0m in \u001b[0;36mModel_Train\u001b[0;34m(arc_func, optim_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mModel_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marc_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3680\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Mahboob/utils.py\u001b[0m in \u001b[0;36mBI_RNN_ARCHITECTURE_SPAMBASE\u001b[0;34m(numSamples, numFeatures)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBI_RNN_ARCHITECTURE_SPAMBASE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    214\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0;34mf'expected ndim={spec.ndim}, found ndim={ndim}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"bidirectional\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 57)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_cCE4G56o5"
      },
      "source": [
        "# Model comparison table with metrics : \n",
        "1. Accuracy\n",
        "  - Train Accuracy from history object\n",
        "  - Test Accuracy from get_Metrics Function in utils\n",
        "2. Loss\n",
        "  - Train loss curve From history object\n",
        "  - Test loss From Model Evaluation  \n",
        "3. Precision, Recall, F1 score\n",
        "4. ROC AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj-7QbBiy-Gh"
      },
      "source": [
        "\"\"\"\n",
        "Model comparison table with metrics : \n",
        "1. Accuracy\n",
        "2. Loss\n",
        "3. Error\n",
        "4. Precision, Recall, F1 score\n",
        "5. ROC AUC\n",
        "\"\"\"\n",
        "Model_comparison = dict()\n",
        "\n",
        "for k,v in enumerate(Model_Architectures.items()):\n",
        "  for index_of_optim, i in enumerate(Optimizer_list):\n",
        "    # print(v[0])\n",
        "    history_curr = Deliverables[\"Optimizer_history\"][v[0]][index_of_optim]\n",
        "    acc_train = history_curr.history[\"accuracy\"][len(history_curr.history[\"accuracy\"])-1]\n",
        "    \n",
        "    y_pred = Deliverables[\"Y_pred\"][v[0]][index_of_optim]\n",
        "    \n",
        "    loss_test = Deliverables[\"Test_Loss\"][v[0]][index_of_optim]\n",
        "    \n",
        "    duration = Deliverables[\"Duration\"][v[0]][index_of_optim]\n",
        "    \n",
        "    precision, recall, f1_score_, acc_test, auc_ = get_Metrics(Y_test, y_pred)\n",
        "    \n",
        "    Model_comparison[f\"{v[0]} {i}\"] = { \n",
        "        \"Train_acc\":acc_train, \n",
        "        \"Test_acc\":acc_test, \n",
        "        \"Test_loss\" : loss_test, \n",
        "        \"auroc\":auc_, \n",
        "        \"f1_score\":f1_score_, \n",
        "        \"precision\":precision, \n",
        "        \"recall\":recall,  \n",
        "        \"duration\": duration \n",
        "        }\n",
        "    print(f\"{v[0]} {i} done\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsqM6gm9-3QA"
      },
      "source": [
        "import re\n",
        "models_metrics = []\n",
        "BRNN_metrics = []\n",
        "BILSTM_metrics = []\n",
        "BIGRU_metrics = []\n",
        "ALL = [models_metrics, BRNN_metrics, BILSTM_metrics, BIGRU_metrics] \n",
        "ALL_names = [\"SpamBase_comparison\", \"SpamBase_BRNN_comparison\", \"SpamBase_Bi-LSTM_comparison\", \"SpamBase_Bi-GRU_comparison\"]\n",
        "\n",
        "for name, model in Model_comparison.items():\n",
        "  precision, recall, f1_score_,  = model[\"precision\"], model[\"recall\"], model[\"f1_score\"]\n",
        "  Test_acc, auc_, Test_loss = model[\"Test_acc\"], model[\"auroc\"], model[\"Test_loss\"],\n",
        "  Train_acc, duration = model[\"Train_acc\"], model[\"duration\"]\n",
        "  models_metrics.append([name, precision, recall, f1_score_, Train_acc, Test_acc, Test_loss, auc_, duration])\n",
        "  if re.match(r\"^BRNN*\", name):\n",
        "    BRNN_metrics.append([name, precision, recall, f1_score_, Train_acc, Test_acc, Test_loss, auc_, duration])\n",
        "  if re.match(r\"^Bi-LSTM*\", name):\n",
        "    BILSTM_metrics.append([name, precision, recall, f1_score_, Train_acc, Test_acc, Test_loss, auc_, duration])\n",
        "  if re.match(r\"^Bi-GRU*\", name):\n",
        "    BIGRU_metrics.append([name, precision, recall, f1_score_, Train_acc, Test_acc, Test_loss, auc_, duration])\n",
        "\n",
        "for i,j in zip(ALL, ALL_names):\n",
        "  df_metrics = pd.DataFrame(i)\n",
        "  df_metrics.columns = ['Model', 'Precision', 'Recall', 'f1 score', \"Train Accuracy\", 'Test Accuracy', 'Loss', 'ROC-AUC', 'Train Time (s)']\n",
        "  df_metrics.sort_values(by = 'Test Accuracy', ascending = False, inplace=True)\n",
        "  df_metrics.reset_index(drop = True, inplace=True)\n",
        "  df_metrics.to_csv(f\"{Comparison}{j}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp-gSWRqzB9A"
      },
      "source": [
        "import pprint\n",
        "pprint.pprint(Model_comparison)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-bneeHIz4V6"
      },
      "source": [
        "df_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zbe-vJa7Dig"
      },
      "source": [
        "# Save the model objects for graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpf1-7wF5LmK"
      },
      "source": [
        "import pickle\n",
        "with open(f\"{Visuals}SpamBase_history.pkl\", \"wb\") as open_file:\n",
        "    pickle.dump(Deliverables[\"Optimizer_history\"], open_file)\n",
        "with open(f\"{Visuals}SpamBase_y_pred.pkl\", \"wb\") as open_file:\n",
        "    pickle.dump(Deliverables[\"Y_pred\"], open_file)\n",
        "with open(f\"{Visuals}SpamBase.pkl\", \"wb\") as open_file:\n",
        "    pickle.dump(Deliverables[\"Y_pred\"], open_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI33E3ri7O1N"
      },
      "source": [
        "Histories = Deliverables[\"Optimizer_history\"].copy()\n",
        "\n",
        "col = ['#feb24c','#fd8d3c','#fc4e2a','#e31a1c','#bd0026','#dadaeb','#bcbddc','#9e9ac8','#807dba','#6a51a3']\n",
        "col_2 = [\"#238b45\", \"#88419d\", \"#2b8cbe\", \"#ef6548\", \"#3690c0\", \"#df65b0\", \"#41b6c4\", \"#6a51a3\", \"#c7e9b4\", \"#74a9cf\"]\n",
        "col_3 = [\"#003f5c\",\"#2f4b7c\",\"#665191\",\"#a05195\",\"#d45087\",\"#f95d6a\",\"#ff7c43\",\"#ffa600\",\"#488f31\",\"#bad0af\"]\n",
        "\n",
        "from cycler import cycler\n",
        "import matplotlib.pyplot as plt\n",
        "custom_cycler = (cycler(color=col_3) +\n",
        "                 cycler(marker=['o', '8', 's', 'p', '*', 'h', 'H', '+', 'x', 'D']))\n",
        "\n",
        "# Global plot parameters\n",
        "plt.rc('lines', linewidth=3, markerfacecolor='white', markeredgewidth=2, markersize=8, linestyle='--')\n",
        "plt.rc('axes', prop_cycle=custom_cycler)\n",
        "font = {'family': 'serif',\n",
        "        'color':  'black',\n",
        "        'weight': 'normal',\n",
        "        'size': 16,\n",
        "        }\n",
        "\n",
        "def plot_metric(metric, history, y_label):\n",
        "\n",
        "  for k, v in enumerate(history.items()):\n",
        "    print(v[0],v[1])\n",
        "    fig, ax = plt.subplots(figsize=(7,7))  # Create a figure and an axes.\n",
        "    #plot legend handles\n",
        "    plt_handles = []\n",
        "    for index_of_optim, i in enumerate(Optimizer_list):\n",
        "      if index_of_optim % 2 == 0:\n",
        "        plot_ = ax.plot(history[v[0]][index_of_optim].history[metric], linestyle='--', label=i)\n",
        "        plt_handles.append(plot_[0])\n",
        "      else: \n",
        "        plot_ = ax.plot(history[v[0]][index_of_optim].history[metric], linestyle='-.', label=i)\n",
        "        plt_handles.append(plot_[0])\n",
        "    ax.set_xlabel('Epochs', fontdict=font)  \n",
        "    ax.set_ylabel(y_label, fontdict=font) \n",
        "    ax.set_title(f\"Spam Base, Model : {v[0]}, {y_label} curve\", fontdict=font) \n",
        "    plt.savefig(f\"{Visuals}Accuracy_Loss/SpamBase_{metric}_{v[0]}.jpeg\")\n",
        "    if k == 0:\n",
        "      fig_legend = plt.figure()\n",
        "      plt.legend(handles = plt_handles, loc = 'center', prop={'family': 'serif', 'size': 16}, frameon=False)\n",
        "      plt.axis(\"off\")\n",
        "      plt.savefig(f\"{Visuals}Accuracy_Loss/SpamBase_{metric}_{v[0]}_legend.jpeg\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "plot_metric(\"accuracy\", Histories, \"Train Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teZ1evyj-Tyh"
      },
      "source": [
        "plot_metric(\"val_accuracy\", Histories, \"Validation Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifOF7EsbRAX"
      },
      "source": [
        "plot_metric(\"loss\", Histories, \"Train Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxInrYfAowKC"
      },
      "source": [
        "plot_metric(\"val_loss\", Histories, \"Validation Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iws1vL0io4oA"
      },
      "source": [
        "# with open(f\"comparison/classifiers/train_test_tf_idf.pkl\", \"rb\") as open_file:\n",
        "#     dataset_processed = pickle.load(open_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhWc0fK5v6Ky"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}